{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from gensim.parsing.porter import PorterStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics import pairwise_distances\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from os import system\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "#clean raw text\n",
    "##\n",
    "\n",
    "#Initialize the porter stemmer and stopwords\n",
    "#stemmer = PorterStemmer()\n",
    "with open('../../Misc/stopwords.txt', 'r') as f:\n",
    "    stops = []\n",
    "    for line in f:\n",
    "        stops.append(line.rstrip())\n",
    "    stops = set(stops)\n",
    "\n",
    "#clean the data and save to disk\n",
    "with open('../../Misc/reviews.txt', 'r') as f:\n",
    "    with open('../../cleaned_reviews.txt', 'w') as cleaned:\n",
    "        for line in f:\n",
    "            line = line.replace('...more', '')\n",
    "            line = line.replace('.', ' ')\n",
    "            #tokens = [ stemmer.stem(word) for word in line.lower().rstrip().split(' ') if word not in stops ]\n",
    "            tokens = [ word.strip() for word in line.lower().rstrip().split(' ') if word not in stops ]\n",
    "            cleaned.write('{}\\n'.format(' '.join(tokens)))\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "# Code for creating the model. After created, then just load the saved model\n",
    "##\n",
    "\n",
    "#Helpful iterator, credit here: https://rare-technologies.com/word2vec-tutorial/\n",
    "class MySentences(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            yield line.rstrip().split()\n",
    "\n",
    "#train word2vec on cleaned data, but do so using the memory saving trick from the link above\n",
    "sentences = MySentences('../../cleaned_reviews.txt')\n",
    "model = Word2Vec(sentences, size = 500, workers = 4)\n",
    "model.save('../../Models/w2v')\n",
    "\n",
    "#Get vectors only from the model and save to disk\n",
    "wv = model.wv\n",
    "wv.save('../../Word_Vectors/wv')\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load word vectors from initial word2vec training\n",
    "wv = KeyedVectors.load('../../Word_Vectors/wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#write master matrix to file\n",
    "with open('../../Pre_Clustering/wvmaster.csv', 'w') as f:\n",
    "\n",
    "    #save word and its vector disk\n",
    "    for word, data in wv.vocab.items():\n",
    "        f.write('{} '.format(word))\n",
    "        f.write('{}\\n'.format(' '.join([ str(element) for element in wv[word] ])))\n",
    "\"\"\"        \n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit code: 0\n"
     ]
    }
   ],
   "source": [
    "#Run the retrofit program (runs in a separate subprocess)\n",
    "print('Exit code: {}'.format(system(\n",
    "    'python \\\n",
    "    ../../retrofitting/retrofit.py \\\n",
    "    -i ../../Pre_Clustering/wvmaster.csv \\\n",
    "    -l ../../Misc/seeds.txt \\\n",
    "    -n 10 \\\n",
    "    -o ../../Pre_Clustering/retrofitted_dirty.txt'\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the vocab->index, index->vocab dictionary, and indexed word vectors and save all to disk\n",
    "vocab2index = {}\n",
    "index2vocab = {}\n",
    "\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'r') as f:\n",
    "    with open('../../Pre_Clustering/retrofitted_clean.txt', 'w') as o:\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            #get the word and its vector separately\n",
    "            splits = line.rstrip().split(' ')\n",
    "            word = splits[0]\n",
    "            vector = splits[1:]\n",
    "            \n",
    "            #build the vocab dictionaries\n",
    "            vocab2index[word] = wv.vocab[word].index\n",
    "            index2vocab[ vocab2index[word] ] = word\n",
    "            \n",
    "            #save the indexed vectors to file for loading later\n",
    "            o.write('{} '.format(vocab2index[word]))\n",
    "            o.write('{}\\n'.format(' '.join(vector)))\n",
    "\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab2index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(index2vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab -> index\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'rb') as f:\n",
    "    vocab2index = pickle.load(f)\n",
    "\n",
    "#index -> vocab\n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'rb') as f:\n",
    "    index2vocab = pickle.load(f)\n",
    "\n",
    "#master numpy matrix with index as first column and word vector as the rest\n",
    "X = np.loadtxt('../../Pre_Clustering/retrofitted_clean.txt', delimiter = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10, 110, 10):\n",
    "\n",
    "    #run k means\n",
    "    kmeans = KMeans(n_clusters = i, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "    #save kmeans\n",
    "    with open('../../KMeans/kmeans_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load kmeans\n",
    "\n",
    "for j in range(10, 110, 10):\n",
    "\n",
    "    with open('../../KMeans/kmeans_{}.pkl'.format(j), 'rb') as f:\n",
    "        kmeans = pickle.load(f)\n",
    "\n",
    "    #attach words to labels\n",
    "    clustered_words = {}\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "    #group words by their labels\n",
    "    fullbags = [ [] for i in range(100) ]\n",
    "    for k, v in clustered_words.items():\n",
    "        fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "    #Sort each cluster and trim to top 20.\n",
    "    wordbags = [ [] for i in range(100) ]\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "        wordbags[i] = fullbags[i][0:20]\n",
    "\n",
    "    with open('../../KMeans/fullbags_{}.pkl'.format(j), 'wb') as f:\n",
    "        pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #save trimmed clusters so we don't have to do anything above again\n",
    "    with open('../../KMeans/wordbags_{}.pkl'.format(j), 'wb') as f:\n",
    "        pickle.dump(wordbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the wordbags and fullbags from file\n",
    "with open('../../KMeans/wordbags.pkl', 'rb') as f:\n",
    "    wordbags = pickle.load(f)\n",
    "    \n",
    "with open('../../KMeans/fullbags.pkl', 'rb') as f:\n",
    "    fullbags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../../Thesaurus/thesaurus.pkl', 'rb') as f:\n",
    "    thesaurus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synSet = set()\n",
    "for k in thesaurus:\n",
    "    synSet.add(k.lower())\n",
    "    \n",
    "vocabSet = set()\n",
    "for k in wv.vocab:\n",
    "    vocabSet.add(k.lower())\n",
    "    \n",
    "intSet = synSet & vocabSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thesausets = [ set(ws) for ws in fullbags ]\n",
    "for i in range(len(thesausets)):\n",
    "    thesausets[i] = thesausets[i] & intSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amenities = set()\n",
    "services = set()\n",
    "location = set()\n",
    "price = set()\n",
    "\n",
    "with open('../../Misc/amenities.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        for i, bag in enumerate(fullbags):\n",
    "            if line.rstrip().lower() in bag:\n",
    "                amenities.add(i)\n",
    "\n",
    "with open('../../Misc/services.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        for i, bag in enumerate(fullbags):\n",
    "            if line.rstrip().lower() in bag:\n",
    "                services.add(i)\n",
    "\n",
    "with open('../../Misc/price.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        for i, bag in enumerate(fullbags):\n",
    "            if line.rstrip().lower() in bag:\n",
    "                price.add(i)\n",
    "\n",
    "with open('../../Misc/location.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        for i, bag in enumerate(fullbags):\n",
    "            if line.rstrip().lower() in bag:\n",
    "                location.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in price:\n",
    "    print(wordbags[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// helpers for creating seed lists //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amenities = set()\n",
    "with open('../../Misc/amenities.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "\n",
    "        word = line.rstrip().lower()\n",
    "        amenities.add(word)\n",
    "\n",
    "        if word in synSet:\n",
    "            amenities = amenities | thesaurus[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "services = set()\n",
    "with open('../../Misc/services.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "\n",
    "        word = line.rstrip().lower()\n",
    "        services.add(word)\n",
    "\n",
    "        if word in synSet:\n",
    "            services = services | thesaurus[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price = set()\n",
    "with open('../../Misc/price.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "\n",
    "        word = line.rstrip().lower()\n",
    "        price.add(word)\n",
    "\n",
    "        if word in synSet:\n",
    "            price = price | thesaurus[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = set()\n",
    "with open('../../Misc/location.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "\n",
    "        word = line.rstrip().lower()\n",
    "        location.add(word)\n",
    "\n",
    "        if word in synSet:\n",
    "            location = location | thesaurus[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../../Misc/seeds.txt', 'w') as f:\n",
    "    \n",
    "    for w1 in amenities:\n",
    "        \n",
    "        f.write('{} '.format(w1))\n",
    "        \n",
    "        for w2 in amenities:\n",
    "            if w1 != w2:\n",
    "                f.write('{} '.format(w2))\n",
    "        \n",
    "        f.write('\\n')\n",
    "    \n",
    "    for w1 in services:\n",
    "        \n",
    "        f.write('{} '.format(w1))\n",
    "        \n",
    "        for w2 in services:\n",
    "            if w1 != w2:\n",
    "                f.write('{} '.format(w2))\n",
    "        \n",
    "        f.write('\\n')\n",
    "    \n",
    "    for w1 in price:\n",
    "        \n",
    "        f.write('{} '.format(w1))\n",
    "        \n",
    "        for w2 in price:\n",
    "            if w1 != w2:\n",
    "                f.write('{} '.format(w2))\n",
    "        \n",
    "        f.write('\\n')\n",
    "    \n",
    "    for w1 in location:\n",
    "        \n",
    "        f.write('{} '.format(w1))\n",
    "        \n",
    "        for w2 in location:\n",
    "            if w1 != w2:\n",
    "                f.write('{} '.format(w2))\n",
    "        \n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
