{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import operator\n",
    "from os import system\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "aspects = ['amenities', 'service', 'price', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "#clean raw text\n",
    "##\n",
    "\n",
    "#Initialize stopwords and regex\n",
    "stops = set(stopwords.words('english'))\n",
    "regex = re.compile(r'[^a-zA-Z0-9\\s]|[\\_\\^\\`\\[\\]\\\\]', re.IGNORECASE)\n",
    "\n",
    "#clean the data and save to disk\n",
    "with codecs.open('../../Misc/reviews.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "    with open('../../cleaned_reviews.txt', 'w') as cleaned:\n",
    "        \n",
    "        for line in f:\n",
    "\n",
    "            #remove non-alphanumeric symbols\n",
    "            line = regex.sub(' ', line.lower().rstrip())\n",
    "\n",
    "            #split into tokens and ignore stopwords\n",
    "            tokens = [ word.strip() for word in line.split(' ') if word not in stops ]\n",
    "\n",
    "            #remove empty elements from the list and stem\n",
    "            tokens = [ word for word in tokens if word != '' ]\n",
    "\n",
    "            #ignore first two elements because they're just identifiers\n",
    "            tokens = tokens[2:]\n",
    "\n",
    "            #write cleaned data to file\n",
    "            if len(tokens) > 0:\n",
    "                cleaned.write('{}\\n'.format(' '.join(tokens)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "# Code for creating the model. After created, then just load the saved model\n",
    "##\n",
    "\n",
    "#Helpful iterator, credit here: https://rare-technologies.com/word2vec-tutorial/\n",
    "class MySentences(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            yield line.rstrip().split()\n",
    "\n",
    "#train word2vec on cleaned data, but do so using the memory saving trick from the link above\n",
    "sentences = MySentences('../../cleaned_reviews.txt')\n",
    "model = Word2Vec(sentences, size = 300, workers = 4)\n",
    "model.save('../../Models/w2v')\n",
    "\n",
    "#Get vectors only from the model and save to disk\n",
    "wv = model.wv\n",
    "wv.save('../../Word_Vectors/wv')\n",
    "\n",
    "#remove model from memory, we only need the wv's\n",
    "del model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load word vectors from initial word2vec training\n",
    "wv = KeyedVectors.load('../../Word_Vectors/wv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#write master matrix to file\n",
    "with open('../../Pre_Clustering/wvmaster.csv', 'w') as f:\n",
    "    \n",
    "    #save word and its vector disk\n",
    "    for word, data in wv.vocab.items():\n",
    "        f.write('{} '.format(word))\n",
    "        f.write('{}\\n'.format(' '.join([ str(element) for element in wv[word] ])))    \n",
    "\n",
    "#set up the index <-> vocab maps\n",
    "index2vocab = {}\n",
    "vocab2index = {}\n",
    "\n",
    "for word in wv.vocab:\n",
    "    vocab2index[word] = wv.vocab[word].index\n",
    "    index2vocab[ vocab2index[word] ] = word\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin default clustering (for seed word creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#write the pre_retrofitted matrix to file\n",
    "with open('../../Pre_Clustering/default_matrix.csv', 'w') as f:\n",
    "    \n",
    "    for word in wv.vocab:\n",
    "        f.write('{} {}\\n'.format(vocab2index[word], ' '.join([ str(element) for element in wv[word] ])))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load the default matrix from file\n",
    "X = np.loadtxt('../../Pre_Clustering/default_matrix.csv', delimiter = ' ')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#use clusters to help find seed words\n",
    "kmeans = KMeans(n_clusters = 100, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "#save kmeans\n",
    "with open('../../KMeans/kmeans_default.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load kmeans\n",
    "with open('../../KMeans/kmeans_default.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "#attach words to labels\n",
    "clustered_words = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "#group words by their labels\n",
    "fullbags = [ [] for i in range(100) ]\n",
    "for k, v in clustered_words.items():\n",
    "    fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "#Sort each cluster\n",
    "for i, bag in enumerate(fullbags):\n",
    "    fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "\n",
    "with open('../../KMeans/fullbags_default.pkl', 'wb') as f:\n",
    "    pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#help finding seed words by using default clusters\n",
    "aspect = 'price'\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "    seeds = set([ word.strip() for word in f.readlines() ])\n",
    "\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'w') as f:\n",
    "    \n",
    "    for bag in fullbags:\n",
    "        \n",
    "        if 'dollar' in bag:\n",
    "            seeds |= set(bag)\n",
    "            \n",
    "    for word in sorted(seeds):\n",
    "        f.write('{}\\n'.format(word))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#trim seed word list to top length by frequency count\n",
    "length = 100\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "    seeds = [ word.strip() for word in f.readlines() if wv.__contains__(word.strip()) ]\n",
    "    seeds = sorted(([ item[0] for item in sorted([ (word, wv.vocab[word].count) for word in seeds ], key = operator.itemgetter(1), reverse = True) ])[:length])\n",
    "\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'w') as f:\n",
    "    for word in seeds:\n",
    "        f.write('{}\\n'.format(word))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('../../Misc/seeds.txt', 'w') as s:\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "            \n",
    "            aspectseeds = [ word.rstrip() for word in f.readlines() ]\n",
    "            for i in range(len(aspectseeds)):\n",
    "                \n",
    "                s.write('{} '.format(aspectseeds[i]))\n",
    "                \n",
    "                for j in range(len(aspectseeds)):\n",
    "                    \n",
    "                    if i != j:\n",
    "                        s.write('{} '.format(aspectseeds[j]))\n",
    "                \n",
    "                s.write('\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End default clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Run the retrofit program (runs in a separate subprocess)\n",
    "print('Exit code: {}'.format(system(\n",
    "    'python \\\n",
    "    ../../retrofitting/retrofit.py \\\n",
    "    -i ../../Pre_Clustering/wvmaster.csv \\\n",
    "    -l ../../Misc/seeds.txt \\\n",
    "    -n 10 \\\n",
    "    -o ../../Pre_Clustering/retrofitted_dirty.txt'\n",
    ")))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#output of retrofit doesn't have a header which we need for loading into gensim\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'w') as f:\n",
    "    \n",
    "    f.write('{} {}\\n'.format(len(wv.vocab), 300))\n",
    "    \n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "\n",
    "del lines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load the retrofitted vectors in as a gensim object\n",
    "wv = KeyedVectors.load_word2vec_format('../../Pre_Clustering/retrofitted_dirty.txt', binary = False)\n",
    "wv.save('../../Word_Vectors/retrofitted_wv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load the retrofitted wordvectors from file\n",
    "wv = KeyedVectors.load('../../Word_Vectors/retrofitted_wv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#create the vocab->index, index->vocab dictionary, and indexed word vectors and save all to disk\n",
    "vocab2index = {}\n",
    "index2vocab = {}\n",
    "\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'r') as f: \n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    with open('../../Pre_Clustering/retrofitted_clean.txt', 'w') as o:\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            \n",
    "            #get the word and its vector separately\n",
    "            splits = line.rstrip().split(' ')\n",
    "            word = splits[0]\n",
    "            vector = splits[1:]\n",
    "            \n",
    "            #build the vocab dictionaries\n",
    "            vocab2index[word] = wv.vocab[word].index\n",
    "            index2vocab[ vocab2index[word] ] = word\n",
    "            \n",
    "            #save the indexed vectors to file for loading later\n",
    "            o.write('{} '.format(vocab2index[word]))\n",
    "            o.write('{}\\n'.format(' '.join(vector)))\n",
    "    \n",
    "    del lines\n",
    "\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab2index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(index2vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Build dictionaries from defaults\n",
    "vocab2index = {}\n",
    "index2vocab = {}\n",
    "\n",
    "for word in wv.vocab:\n",
    "    vocab2index[word] = wv.vocab[word].index\n",
    "    index2vocab[ vocab2index[word] ] = word\n",
    "    \n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab2index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(index2vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many k\n",
    "numk = 900\n",
    "\n",
    "#load the retrofitted wordvectors from file\n",
    "#wv = KeyedVectors.load('../../Word_Vectors/retrofitted_wv')\n",
    "wv = KeyedVectors.load('../../Word_Vectors/wv')\n",
    "\n",
    "#vocab -> index\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'rb') as f:\n",
    "    vocab2index = pickle.load(f)\n",
    "\n",
    "#index -> vocab\n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'rb') as f:\n",
    "    index2vocab = pickle.load(f)\n",
    "\n",
    "#master numpy matrix with index as first column and word vector as the rest\n",
    "#X = np.loadtxt('../../Pre_Clustering/retrofitted_clean.txt', delimiter = ' ')\n",
    "X = np.loadtxt('../../Pre_Clustering/default_matrix.csv', delimiter = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/// For gross scoring ///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#find the best scoring k from 100 - 1000 (every 100)\n",
    "for numk in range(100, 1100, 100):\n",
    "    \n",
    "    #kmeans model\n",
    "    kmeans = KMeans(n_clusters = numk, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "    \n",
    "    #save kmeans\n",
    "    with open('../../KMeans/kmeans_{}.pkl'.format(numk), 'wb') as f:\n",
    "        pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #attach words to labels\n",
    "    clustered_words = {}\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "    #group words by their labels\n",
    "    fullbags = [ [] for i in range(numk) ]\n",
    "    for k, v in clustered_words.items():\n",
    "        fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "    #Sort each cluster\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "    \n",
    "    #save clustered words\n",
    "    with open('../../KMeans/fullbags_{}.pkl'.format(numk), 'wb') as f:\n",
    "        pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #score for this k\n",
    "    kscore = 0.0\n",
    "    numclusters = 0\n",
    "\n",
    "    #score the clustering\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        setbag = set(bag)\n",
    "        clusterscore = 0.0\n",
    "        numaspects = 0\n",
    "\n",
    "        #go through every aspect\n",
    "        for aspect in aspects:\n",
    "            with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "                #seed words\n",
    "                temp = set([ line.rstrip() for line in f ])\n",
    "\n",
    "                #don't worry about clusters that don't ahve any aspect seed words\n",
    "                if len(temp & setbag) > 0:\n",
    "                    clusterscore += wv.n_similarity(temp & setbag, setbag)\n",
    "                    numaspects += 1\n",
    "\n",
    "        #only care about clusters with aspects\n",
    "        if numaspects > 0:\n",
    "            numclusters += 1\n",
    "            kscore += clusterscore / numaspects\n",
    "\n",
    "    #average score\n",
    "    kscore /= numclusters\n",
    "\n",
    "    with open('../../Cluster_Metrics/scores.txt', 'a') as f:\n",
    "        f.write('k = {} {}\\n'.format(numk, kscore))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/// end gross scoring ///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#run k means\n",
    "kmeans = KMeans(n_clusters = numk, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "#save kmeans\n",
    "with open('../../KMeans/kmeans_{}.pkl'.format(numk), 'wb') as f:\n",
    "    pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load kmeans\n",
    "with open('../../KMeans/kmeans_{}.pkl'.format(numk), 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "#attach words to labels\n",
    "clustered_words = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "#group words by their labels\n",
    "fullbags = [ [] for i in range(numk) ]\n",
    "for k, v in clustered_words.items():\n",
    "    fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "#Sort each cluster\n",
    "for i, bag in enumerate(fullbags):\n",
    "    fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "\n",
    "with open('../../KMeans/fullbags_{}.pkl'.format(numk), 'wb') as f:\n",
    "    pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numk = 900\n",
    "\n",
    "#load the wordbags and fullbags from file\n",
    "#with open('../../KMeans/fullbags_{}.pkl'.format(numk), 'rb') as f:\n",
    "with open('../../Validation/fullbags_{}.pkl'.format(numk), 'rb') as f:\n",
    "    fullbags = pickle.load(f)\n",
    "\n",
    "#wordbags is top 20 words from each cluster\n",
    "wordbags = [ bag[:20] for bag in fullbags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#for scoring\n",
    "#using gensim's similarity method, find the similarity between the seed words\n",
    "#in a cluster and all of the words in the cluster\n",
    "confidence = [ { aspect: 0.0 for aspect in aspects } for i in range(numk) ]\n",
    "\n",
    "for aspect in aspects:\n",
    "    with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "        temp = set()\n",
    "        for line in f:\n",
    "            temp.add(line.rstrip().lower())\n",
    "\n",
    "        for i, bag in enumerate(fullbags):\n",
    "\n",
    "            setbag = set(bag)\n",
    "\n",
    "            if len(temp & setbag) > 0:\n",
    "                confidence[i][aspect] = wv.n_similarity(temp & setbag, setbag)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group all aspect words into their respective sets and find mean vectors\n",
    "aspectsets = { aspect: [] for aspect in aspects }\n",
    "\n",
    "for aspect in aspects:\n",
    "    with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "        temp = set()\n",
    "        for line in f:\n",
    "            temp.add(line.rstrip().lower())\n",
    "\n",
    "        for i, bag in enumerate(fullbags):\n",
    "\n",
    "            setbag = set(bag)\n",
    "\n",
    "            if len(temp & setbag) > 0:\n",
    "                \n",
    "                aspectsets[aspect].append(np.mean([ wv[word] for word in setbag ], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Master dictionary that contains every review broken down by sentence,\n",
    "#the aspect(s) present in the sentence, and the sentiment score of the sentence\n",
    "docs = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "stops = set(stopwords.words('english'))\n",
    "regex = re.compile(r'[^a-zA-Z0-9\\s.!?]|[\\_\\^\\`\\[\\]\\\\]', re.IGNORECASE)\n",
    "splitregex = re.compile(r'[.|?|!]', re.IGNORECASE)\n",
    "\n",
    "#clean the data and save to disk\n",
    "with codecs.open('../../Misc/reviews.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "    \n",
    "    filenum = 1\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "\n",
    "        #remove non-alphanumeric symbols\n",
    "        line = regex.sub(' ', line.lower().rstrip())\n",
    "        \n",
    "        #splitup by word\n",
    "        words = line.split(' ')\n",
    "        \n",
    "        #remove first two words, but save in own variable\n",
    "        review_number, line = '_'.join(words[:2]), ' '.join(words[2:])\n",
    "\n",
    "        #split into tokens and ignore stopwords\n",
    "        sentences = [ sentence.strip() for sentence in splitregex.split(line) ]\n",
    "        sentences = [ ' '.join([ word.strip() for word in sentence.split(' ') if word not in stops and wv.__contains__(word.strip()) ]) for sentence in sentences ]\n",
    "        \n",
    "        #remove empty sentences\n",
    "        sentences = [ sentence for sentence in sentences if sentence != '' ]\n",
    "\n",
    "        #write cleaned data to file\n",
    "        if len(sentences) > 0:\n",
    "            \n",
    "            #add doc to list\n",
    "            docs.append({'database_id': review_number, 'array_id': i, 'score': 0.0, 'data': [ { 'doc_id': i, 'sentence': sentence, 'sentiment': analyzer.polarity_scores(sentence), 'aspect': [] } for sentence in sentences ] })\n",
    "            \n",
    "        #save in 5 pieces\n",
    "        if i % (1373102 // 5) == 0 and i != 0:\n",
    "            \n",
    "            #save to disk\n",
    "            with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format(filenum), 'wb') as f:\n",
    "                pickle.dump(docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            #updates\n",
    "            filenum += 1\n",
    "            del docs\n",
    "            docs = []\n",
    "            \n",
    "if len(docs) > 0:\n",
    "    \n",
    "    #save to disk\n",
    "    with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format(filenum + 1), 'wb') as f:\n",
    "        pickle.dump(docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#return cosine similarity of two vectors (need to be oriented vertically)\n",
    "def similarity(a, b):\n",
    "    \n",
    "    return a.T.dot(b) / (np.sqrt(a.T.dot(a)) * np.sqrt(b.T.dot(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#go through each set of documents\n",
    "for i in range(2, 7):\n",
    "\n",
    "    #we can only work with parts of the overall document set at a time\n",
    "    with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format(i), 'rb') as f:\n",
    "\n",
    "        #load the documents being considered into memory\n",
    "        docs = pickle.load(f)\n",
    "\n",
    "        #go through every review\n",
    "        for doc in docs:\n",
    "\n",
    "            #go through every sentence in the review\n",
    "            for sentdata in doc['data']:\n",
    "\n",
    "                #find mean of the sentence\n",
    "                mean = np.mean([ wv[word] for word in sentdata['sentence'].split(' ') ], axis = 0)\n",
    "\n",
    "                #find the aspect scores\n",
    "                sentdata['aspect'] = [ (aspect, max([ similarity(mean, bagmean) for bagmean in aspectsets[aspect] ])) for aspect in aspects ]\n",
    "\n",
    "    #save to disk\n",
    "    with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format( 1 ), 'wb') as f:\n",
    "        pickle.dump(docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #clear memory up\n",
    "    del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#go through each set of documents\n",
    "for i in range(1, 7):\n",
    "    \n",
    "    #we can only work with part of the whole document set at a time\n",
    "    with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format(i), 'rb') as f:\n",
    "        \n",
    "        #load the documents being considered into memory\n",
    "        docs = pickle.load(f)\n",
    "    \n",
    "        #score each doc\n",
    "        for doc in docs:\n",
    "            \n",
    "            #tracks aspects present in the review\n",
    "            aspcnt = { aspect: 0 for aspect in aspects }\n",
    "            \n",
    "            #go through each sentence\n",
    "            for data in doc['data']:\n",
    "                \n",
    "                #go through each aspect\n",
    "                for asp in data['aspect']:\n",
    "                    \n",
    "                    #only count an aspect as being present if similarity is > 0.5\n",
    "                    if asp[1] >= 0.5:\n",
    "                        aspcnt[ asp[0] ] += 1\n",
    "            \n",
    "            #score is sum of absolute value of compound sentiment + sum of aspect count for the whole review\n",
    "            doc['score'] = sum([ abs(snt['sentiment']['compound']) for snt in doc['data'] ]) + sum([aspcnt[aspect] for aspect in aspects ])\n",
    "\n",
    "    #save\n",
    "    with open('../../Aspect_Dictionary/aspect_dictionary_{}.pkl'.format(i), 'wb') as f:\n",
    "        pickle.dump(docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #clear up some memory\n",
    "    del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ids = sorted([ (doc['array_id'], doc['score']) for doc in docs ], key = operator.itemgetter(1), reverse  = True)\n",
    "\n",
    "with codecs.open('../../Misc/reviews.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "    lines = []\n",
    "    for i in range(numlines):\n",
    "        lines.append(f.readline().rstrip())\n",
    "\n",
    "with open('../../Results/results_{}.txt'.format(numlines), 'w') as f:\n",
    "    for i in range(5):\n",
    "        f.write('{}\\n\\n'.format(lines[ ids[i][0] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Begin gross k scoring //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for numk in range(100, 1100, 100):\n",
    "    \n",
    "    #kmeans model\n",
    "    kmeans = KMeans(n_clusters = numk, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "    #attach words to labels\n",
    "    clustered_words = {}\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "    #group words by their labels\n",
    "    fullbags = [ [] for i in range(numk) ]\n",
    "    for k, v in clustered_words.items():\n",
    "        fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "    #Sort each cluster\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "\n",
    "    #score for this k\n",
    "    kscore = 0.0\n",
    "    numclusters = 0\n",
    "\n",
    "    #score the clustering\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        setbag = set(bag)\n",
    "        clusterscore = 0.0\n",
    "        numaspects = 0\n",
    "\n",
    "        #go through every aspect\n",
    "        for aspect in aspects:\n",
    "            with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "                #seed words\n",
    "                temp = set([ line.rstrip() for line in f ])\n",
    "\n",
    "                #don't worry about clusters that don't ahve any aspect seed words\n",
    "                if len(temp & setbag) > 0:\n",
    "                    clusterscore += wv.n_similarity(temp & setbag, setbag)\n",
    "                    numaspects += 1\n",
    "\n",
    "        #only care about clusters with aspects\n",
    "        if numaspects > 0:\n",
    "            numclusters += 1\n",
    "            kscore += clusterscore / numaspects\n",
    "\n",
    "    #average score\n",
    "    kscore /= numclusters\n",
    "\n",
    "    with open('../../Cluster_Metrics/scores.txt', 'a') as f:\n",
    "        f.write('k = {} {}\\n'.format(numk, kscore))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// end gross k scoring //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// begin seed list creation helpers //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('../../Thesaurus/thesaurus.pkl', 'rb') as f:\n",
    "    thesaurus = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "synSet = set()\n",
    "for k in thesaurus:\n",
    "    synSet.add(k.lower())\n",
    "    \n",
    "vocabSet = set()\n",
    "for k in wv.vocab:\n",
    "    vocabSet.add(k.lower())\n",
    "    \n",
    "intSet = synSet & vocabSet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "thesausets = [ set(ws) for ws in fullbags ]\n",
    "for i in range(len(thesausets)):\n",
    "    thesausets[i] = thesausets[i] & intSet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "aspect = 'service'\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "    words  = set([ line.rstrip() for line in f.readlines() ])\n",
    "\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'w') as f:\n",
    "    \n",
    "    for bag in fullbags:\n",
    "        if 'customer' in bag:\n",
    "            words |= set(bag)\n",
    "            \n",
    "    for word in sorted(words):\n",
    "        f.write('{}\\n'.format(word))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "    words  = set([ line.rstrip() for line in f.readlines() ])\n",
    "\n",
    "with open('../../Misc/{}.txt'.format(aspect), 'w') as f:\n",
    "    for word in sorted([ item[0] for item in sorted([ (word, wv.vocab[word].count) for word in words], key = operator.itemgetter(1), reverse = True)[:100] ]):\n",
    "        f.write('{}\\n'.format(word))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#write seeds as adjacency list\n",
    "with open('../../Misc/seeds.txt', 'w') as f:\n",
    "    for aspect in aspects:\n",
    "        with open('../../Misc/{}.txt'.format(aspect), 'r') as a:\n",
    "            words = [ line.rstrip() for line in a ]\n",
    "            for i in range(len(words)):\n",
    "                f.write('{} '.format(words[i]))\n",
    "                for j in range(len(words)):\n",
    "                    if i != j:\n",
    "                        f.write('{} '.format(words[j]))\n",
    "                f.write('\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// end seed list creation helpers //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// default vector scoring //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load word vectors from initial word2vec training\n",
    "wv = KeyedVectors.load('../../Word_Vectors/wv')\n",
    "\n",
    "vocab2index = {}\n",
    "index2vocab = {}\n",
    "\n",
    "with open('../../Validation/wvmaster.csv', 'w') as f:\n",
    "    \n",
    "    #save word and its vector disk\n",
    "    for word, data in wv.vocab.items():\n",
    "        vocab2index[word] = wv.vocab[word].index\n",
    "        index2vocab[ vocab2index[word] ] = word\n",
    "        \n",
    "        f.write('{} '.format(wv.vocab[word].index))\n",
    "        f.write('{}\\n'.format(' '.join([ str(element) for element in wv[word] ])))\n",
    "\n",
    "X = np.loadtxt('../../Validation/wvmaster.csv', delimiter = ' ')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for numk in range(100, 1100, 100):\n",
    "\n",
    "    kmeans = KMeans(n_clusters = numk, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "    #attach words to labels\n",
    "    clustered_words = {}\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "    #group words by their labels\n",
    "    fullbags = [ [] for i in range(numk) ]\n",
    "    for k, v in clustered_words.items():\n",
    "        fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "    #Sort each cluster\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "    \n",
    "    #save fullbags\n",
    "    with open('../../Validation/fullbags_{}.pkl'.format(numk), 'wb') as f:\n",
    "        pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #score for this k\n",
    "    kscore = 0.0\n",
    "    numclusters = 0\n",
    "\n",
    "    #score the clustering\n",
    "    for i, bag in enumerate(fullbags):\n",
    "        setbag = set(bag)\n",
    "        clusterscore = 0.0\n",
    "        numaspects = 0\n",
    "\n",
    "        #go through every aspect\n",
    "        for aspect in aspects:\n",
    "            with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "                #seed words\n",
    "                temp = set([ line.rstrip() for line in f ])\n",
    "\n",
    "                #don't worry about clusters that don't ahve any aspect seed words\n",
    "                if len(temp & setbag) > 0:\n",
    "                    clusterscore += wv.n_similarity(temp & setbag, setbag)\n",
    "                    numaspects += 1\n",
    "\n",
    "        #only care about clusters with aspects\n",
    "        if numaspects > 0:\n",
    "            numclusters += 1\n",
    "            kscore += clusterscore / numaspects\n",
    "\n",
    "    #average score\n",
    "    kscore /= numclusters\n",
    "\n",
    "    with open('../../Validation/scores.txt', 'a') as f:\n",
    "        f.write('k = {} {}\\n'.format(numk, kscore))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// end default scoring //"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
