{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run if raw dataset has changed\n",
    "\"\"\"\n",
    "##\n",
    "#clean raw text\n",
    "##\n",
    "\n",
    "#Initialize the porter stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "with open('../Misc/stopwords.txt', 'r') as f:\n",
    "    stops = []\n",
    "    for line in f:\n",
    "        stops.append(line.rstrip())\n",
    "    stops = set(stops)\n",
    "\n",
    "#clean the data and save to disk\n",
    "cleaned = open('../cleaned_reviews.txt', 'w')\n",
    "\n",
    "with open('reviews.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('...more', '')\n",
    "        line = line.replace('.', ' ')\n",
    "        tokens = [ stemmer.stem(word) for word in line.lower().rstrip().split(' ') if word not in stops ]\n",
    "        cleaned.write('{}\\n'.format(' '.join(tokens)))\n",
    "\n",
    "cleaned.close()\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run if cleaned dataset has changed or you want a different word2vec model\n",
    "\"\"\"\n",
    "##\n",
    "# Code for creating the model. After created, then just load the saved model\n",
    "##\n",
    "\n",
    "#Helpful iterator, credit here: https://rare-technologies.com/word2vec-tutorial/\n",
    "class MySentences(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            yield line.rstrip().split()\n",
    "\n",
    "#train word2vec on cleaned data, but do so using the memory saving trick from the link above\n",
    "sentences = MySentences('../cleaned_reviews.txt')\n",
    "model = Word2Vec(sentences, size = 600, workers = 4)\n",
    "model.save('../Models/w2v_600d')\n",
    "\n",
    "#Get vectors only from the model and save to disk\n",
    "wv = model.wv\n",
    "wv.save('../Word_Vectors/wv_600d')\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run if word2vec model has changed\n",
    "\"\"\"\n",
    "#dict mapping ids to words\n",
    "word_ids = {}\n",
    "\n",
    "#write master matrix to file\n",
    "with open('../Pre_Clustering/wvmaster_600d.csv', 'w') as f:\n",
    "\n",
    "    #create matrix for clustering, 1st column is id, save to disk\n",
    "    for word, data in wv.vocab.items():\n",
    "        word_ids[ str(data.index) ] = word\n",
    "        temp = np.array([ data.index ])\n",
    "        temp = np.append(temp, wv[word])\n",
    "        temp = ','.join([ str(element) for element in temp ])\n",
    "        f.write('{}\\n'.format(temp))\n",
    "\n",
    "#Save id dictionary to disk\n",
    "with open('../Pre_Clustering/wordids_600d.pkl', 'wb') as f:\n",
    "    pickle.dump(word_ids, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uncomment if word2vec model has changed\n",
    "\n",
    "#load numpy data\n",
    "X = np.loadtxt('../Pre_Clustering/wvmaster_1500d.csv', delimiter = ',')\n",
    "\n",
    "#load dictionary data\n",
    "word_ids = {}\n",
    "with open('../Pre_Clustering/wordids_1500d.pkl', 'rb') as f:\n",
    "    word_ids = pickle.load(f)\n",
    "\n",
    "#load the word2vec model\n",
    "w2v = Word2Vec.load('../Models/w2v_1500d')\n",
    "    \n",
    "#load word vectors from initial word2vec training\n",
    "wv = KeyedVectors.load('../Word_Vectors/wv_1500d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run if word2vec model has changed\n",
    "\"\"\"\n",
    "#run k means\n",
    "kmeans = KMeans(n_clusters = 100, random_state = 0).fit(X[:, 1:X.shape[1] ])\n",
    "\n",
    "#save kmeans\n",
    "with open('../KMeans/kmeans_600d.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run if kmeans has changed\n",
    "\"\"\"\n",
    "#load kmeans\n",
    "with open('../KMeans/kmeans_600d.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "#attach words to labels\n",
    "clustered_words = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    clustered_words[ word_ids[str(int(X[i, 0]))] ] = label\n",
    "\n",
    "#group words by their labels\n",
    "wordbags = [ [] for i in range(100) ]\n",
    "for k, v in clustered_words.items():\n",
    "    wordbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "    \n",
    "#Sort each cluster and trim to top 20.\n",
    "fullbags = [''] * 100\n",
    "for i, bag in enumerate(wordbags):\n",
    "    wordbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "    fullbags[i] = wordbags[i]\n",
    "    wordbags[i] = wordbags[i][0:20]\n",
    "    \n",
    "with open('../KMeans/fullbags_600d.pkl' , 'wb') as f:\n",
    "    pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#save trimmed clusters so we don't have to do anything above again\n",
    "with open('../KMeans/wordbags_600d.pkl', 'wb') as f:\n",
    "    pickle.dump(wordbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the wordbags and fullbags from file\n",
    "with open('../KMeans/wordbags_1500d.pkl', 'rb') as f:\n",
    "    wordbags = pickle.load(f)\n",
    "    \n",
    "with open('../KMeans/fullbags_1500d.pkl', 'rb') as f:\n",
    "    fullbags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#save the top 20 words from each cluster to text file\n",
    "with open('../Pre_Paircounting/Top20_600d.txt', 'w') as f:\n",
    "    for bag in wordbags:\n",
    "        for word in bag:\n",
    "            f.write('{}\\n'.format(word))\n",
    "        f.write('###\\n')\n",
    "\"\"\"\n",
    "print('Comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/////////////////////////////////////////////////////\n",
    "// Run Up to This point before running C++ program //\n",
    "/////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get paircounts for wordvec from file\n",
    "with open('../Cluster_Metrics/paircounts_1500d.txt', 'r') as f:\n",
    "    paircounts = []\n",
    "    for line in f:\n",
    "        paircounts.append(int(line.rstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combination formula\n",
    "def nCr(n, r):\n",
    "    f = math.factorial\n",
    "    if n < r:\n",
    "        return 1\n",
    "    else:\n",
    "        return f(n) // f(r) // f(n - r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = [ (i, float(paircounts[i]) / nCr(len(wordbags[i]), 2)) for i in range(len(wordbags)) ]\n",
    "clusters = sorted(clusters, key = lambda item: item[1], reverse = True)\n",
    "v = np.array([ item[1] for item in clusters ])\n",
    "\n",
    "#with open('../Cluster_Metrics/Stats.txt', 'a') as f:\n",
    "#    f.write('Word2Vec with 600 dim: Avg = {:.3f}; Std Dev = {:.3f}\\n'.format(np.mean(v), np.std(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////////////\n",
    "// END \"CLUSTER GOODNESS\" MEASUREMENT //\n",
    "////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying seed word stuff\n",
    "with open('../Misc/amenities_keywords.txt', 'r') as f:\n",
    "    amenities = [ line.rstrip() for line in f ]\n",
    "\n",
    "with open('../Misc/service_keywords.txt', 'r') as f:\n",
    "    services = [ line.rstrip() for line in f ]\n",
    "\n",
    "#Initialize the porter stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "with open('../Misc/stopwords.txt', 'r') as f:\n",
    "    stops = []\n",
    "    for line in f:\n",
    "        stops.append(line.rstrip())\n",
    "    stops = set(stops)\n",
    "    \n",
    "amenities = [ stemmer.stem(word.lower()) for word in amenities if word not in stops ]\n",
    "services = [ stemmer.stem(word.lower()) for word in services if word not in stops ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback', 'appear', 'effici', 'prioriti', 'teamwork', 'care', 'peopl', 'empathi', 'flexibl', 'posit', 'qualiti', 'custom', 'listen', 'humor', 'respons', 'patient', 'valu', 'timeli', 'friendli', 'tact'}\n"
     ]
    }
   ],
   "source": [
    "#find the clusters that are most similar to the seed words\n",
    "amenity_similars = set([])\n",
    "for i, bag in enumerate(fullbags):\n",
    "    if len(set(amenities) & set(bag)) / float(len(set(amenities))) > 0.5:\n",
    "        amenity_similars.add(i)\n",
    "\n",
    "service_similars = set([])\n",
    "for i, bag in enumerate(fullbags):\n",
    "    if len(set(services) & set(bag)) / float(len(set(services))) > 0.5:\n",
    "        print(set(services) & set(bag))\n",
    "        service_similars.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Misc/reviews.txt', 'r') as f:\n",
    "    testreviews = []\n",
    "    for i in range(25):\n",
    "        line = f.readline().lower().rstrip()\n",
    "        line = line.replace('...more', '')\n",
    "        line = line.replace('.', '. ')\n",
    "        sentences = line.split('.')\n",
    "        sentences = [ sentence.strip() for sentence in sentences if sentence not in [' '] ]\n",
    "        testreviews.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in testreviews[9]:\n",
    "    tokens = [ stemmer.stem(word) for word in sentence.split(' ') if word not in stops ]\n",
    "    for i in service_similars:\n",
    "        if len(set(tokens) & set(fullbags[i])) / float(len(tokens)) > 0.5:\n",
    "            print('Service: {}'.format(' '.join(tokens)))\n",
    "            \n",
    "    for i in amenity_similars:\n",
    "        if len(set(tokens) & set(fullbags[i])) / float(len(tokens)) > 0.5:\n",
    "            print('Amenity: {}'.format(' '.join(tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
