{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from gensim.parsing.porter import PorterStemmer\n",
    "#from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics import pairwise_distances\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from os import system\n",
    "%matplotlib inline\n",
    "\n",
    "aspects = ['amenities', 'service', 'price', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "#clean raw text\n",
    "##\n",
    "\n",
    "#Initialize the porter stemmer and stopwords\n",
    "#stemmer = PorterStemmer()\n",
    "with open('../../Misc/stopwords.txt', 'r') as f:\n",
    "    stops = []\n",
    "    for line in f:\n",
    "        stops.append(line.rstrip())\n",
    "    stops = set(stops)\n",
    "\n",
    "#clean the data and save to disk\n",
    "with open('../../Misc/reviews.txt', 'r') as f:\n",
    "    with open('../../cleaned_reviews.txt', 'w') as cleaned:\n",
    "        for line in f:\n",
    "            line = line.replace('...more', '')\n",
    "            line = line.replace('.', ' ')\n",
    "            #tokens = [ stemmer.stem(word) for word in line.lower().rstrip().split(' ') if word not in stops ]\n",
    "            tokens = [ word.strip() for word in line.lower().rstrip().split(' ') if word not in stops ]\n",
    "            cleaned.write('{}\\n'.format(' '.join(tokens)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##\n",
    "# Code for creating the model. After created, then just load the saved model\n",
    "##\n",
    "\n",
    "#Helpful iterator, credit here: https://rare-technologies.com/word2vec-tutorial/\n",
    "class MySentences(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            yield line.rstrip().split()\n",
    "\n",
    "#train word2vec on cleaned data, but do so using the memory saving trick from the link above\n",
    "sentences = MySentences('../../cleaned_reviews.txt')\n",
    "model = Word2Vec(sentences, size = 300, workers = 4)\n",
    "model.save('../../Models/w2v')\n",
    "\n",
    "#Get vectors only from the model and save to disk\n",
    "wv = model.wv\n",
    "wv.save('../../Word_Vectors/wv')\n",
    "\n",
    "#remove model from memory, we only need the wv's\n",
    "del model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load word vectors from initial word2vec training\n",
    "wv = KeyedVectors.load('../../Word_Vectors/wv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"#write master matrix to file\n",
    "with open('../../Pre_Clustering/wvmaster.csv', 'w') as f:\n",
    "    \n",
    "    #save word and its vector disk\n",
    "    for word, data in wv.vocab.items():\n",
    "        f.write('{} '.format(word))\n",
    "        f.write('{}\\n'.format(' '.join([ str(element) for element in wv[word] ])))    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Run the retrofit program (runs in a separate subprocess)\n",
    "print('Exit code: {}'.format(system(\n",
    "    'python \\\n",
    "    ../../retrofitting/retrofit.py \\\n",
    "    -i ../../Pre_Clustering/wvmaster.csv \\\n",
    "    -l ../../Misc/seeds.txt \\\n",
    "    -n 10 \\\n",
    "    -o ../../Pre_Clustering/retrofitted_dirty.txt'\n",
    ")))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#output of retrofit doesn't have a header which we need for loading into gensim\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'w') as f:\n",
    "    \n",
    "    f.write('{} {}\\n'.format(len(wv.vocab), 300))\n",
    "    \n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "\n",
    "del lines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load the retrofitted vectors in as a gensim object\n",
    "wv = KeyedVectors.load_word2vec_format('../../Pre_Clustering/retrofitted_dirty.txt', binary = False)\n",
    "wv.save('../../Word_Vectors/retrofitted_wv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load the retrofitted wordvectors from file\n",
    "wv = KeyedVectors.load('../../Word_Vectors/retrofitted_wv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#create the vocab->index, index->vocab dictionary, and indexed word vectors and save all to disk\n",
    "vocab2index = {}\n",
    "index2vocab = {}\n",
    "\n",
    "with open('../../Pre_Clustering/retrofitted_dirty.txt', 'r') as f: \n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    with open('../../Pre_Clustering/retrofitted_clean.txt', 'w') as o:\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            \n",
    "            #get the word and its vector separately\n",
    "            splits = line.rstrip().split(' ')\n",
    "            word = splits[0]\n",
    "            vector = splits[1:]\n",
    "            \n",
    "            #build the vocab dictionaries\n",
    "            vocab2index[word] = wv.vocab[word].index\n",
    "            index2vocab[ vocab2index[word] ] = word\n",
    "            \n",
    "            #save the indexed vectors to file for loading later\n",
    "            o.write('{} '.format(vocab2index[word]))\n",
    "            o.write('{}\\n'.format(' '.join(vector)))\n",
    "    \n",
    "    del lines\n",
    "\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab2index, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(index2vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many k\n",
    "numk = 1000\n",
    "\n",
    "#load the retrofitted wordvectors from file\n",
    "wv = KeyedVectors.load('../../Word_Vectors/retrofitted_wv')\n",
    "\n",
    "#vocab -> index\n",
    "with open('../../Vector_Tracking/vocab2index.pkl', 'rb') as f:\n",
    "    vocab2index = pickle.load(f)\n",
    "\n",
    "#index -> vocab\n",
    "with open('../../Vector_Tracking/index2vocab.pkl', 'rb') as f:\n",
    "    index2vocab = pickle.load(f)\n",
    "\n",
    "#master numpy matrix with index as first column and word vector as the rest\n",
    "X = np.loadtxt('../../Pre_Clustering/retrofitted_clean.txt', delimiter = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#run k means\n",
    "kmeans = KMeans(n_clusters = numk, random_state = 0, n_jobs = 4).fit(X[:, 1:])\n",
    "\n",
    "#save kmeans\n",
    "with open('../../KMeans/kmeans_{}.pkl'.format(numk), 'wb') as f:\n",
    "    pickle.dump(kmeans, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load kmeans\n",
    "with open('../../KMeans/kmeans_{}.pkl'.format(numk), 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "#attach words to labels\n",
    "clustered_words = {}\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    clustered_words[ index2vocab[int(X[i, 0])] ] = label\n",
    "\n",
    "#group words by their labels\n",
    "fullbags = [ [] for i in range(numk) ]\n",
    "for k, v in clustered_words.items():\n",
    "    fullbags[int(v)].append( (k, wv.vocab[k].count) )\n",
    "\n",
    "#Sort each cluster and trim to top 20.\n",
    "wordbags = [ [] for i in range(numk) ]\n",
    "for i, bag in enumerate(fullbags):\n",
    "    fullbags[i] = [ item[0] for item in sorted(bag, key = operator.itemgetter(1), reverse = True) ]\n",
    "    wordbags[i] = fullbags[i][0:20]\n",
    "\n",
    "with open('../../KMeans/fullbags_{}.pkl'.format(numk), 'wb') as f:\n",
    "    pickle.dump(fullbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#save trimmed clusters so we don't have to do anything above again\n",
    "with open('../../KMeans/wordbags_{}.pkl'.format(numk), 'wb') as f:\n",
    "    pickle.dump(wordbags, f, pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the wordbags and fullbags from file\n",
    "with open('../../KMeans/wordbags_{}.pkl'.format(numk), 'rb') as f:\n",
    "    wordbags = pickle.load(f)\n",
    "\n",
    "with open('../../KMeans/fullbags_{}.pkl'.format(numk), 'rb') as f:\n",
    "    fullbags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using gensm's similarity method, find the similarity between the seed words\n",
    "#in a cluster and all of the words in the cluster\n",
    "confidence = [ { aspect: 0.0 for aspect in aspects } for i in range(numk) ]\n",
    "\n",
    "for aspect in aspects:\n",
    "    with open('../../Misc/{}.txt'.format(aspect), 'r') as f:\n",
    "\n",
    "        temp = set()\n",
    "        for line in f:\n",
    "            temp.add(line.rstrip().lower())\n",
    "\n",
    "        for i, bag in enumerate(fullbags):\n",
    "\n",
    "            setbag = set(bag)\n",
    "\n",
    "            if len(temp & setbag) > 0:\n",
    "                confidence[i][aspect] = wv.n_similarity(temp & setbag, setbag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the labeled clusters and the confidence level we have that it is said label\n",
    "#so far, best scoring k is 1000\n",
    "for i, c in enumerate(confidence):\n",
    "\n",
    "    go = False\n",
    "    for aspect in aspects:\n",
    "        \n",
    "        if c[aspect] > 0:\n",
    "            go = True\n",
    "            break\n",
    "\n",
    "    if go:\n",
    "        \n",
    "        print('C{}'.format(i), end = ' ')\n",
    "        \n",
    "        for aspect in aspects:\n",
    "            \n",
    "            if c[aspect] > 0:\n",
    "                print('{}: {:.4f}'.format(aspect, c[aspect]), end = ' ')\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Begin gross k scoring //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#read each cluster's data into memory\n",
    "with open('../../Cluster_Metrics/overlaps.txt', 'r') as f:\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    clusters = []\n",
    "    temp = []\n",
    "    for i in range(1, len(lines)):\n",
    "        \n",
    "        if lines[i].rstrip()[0:3] == '###':\n",
    "            clusters.append(temp)\n",
    "            temp = []\n",
    "        else:\n",
    "            temp.append(lines[i].rstrip())\n",
    "    \n",
    "    clusters.append(temp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kdata = {}\n",
    "for i, c in enumerate(clusters):\n",
    "    \n",
    "    kdata[i] = {}\n",
    "    \n",
    "    for line in c:\n",
    "        data = line.split(',')\n",
    "        kdata[i][ data[0] ] = {}\n",
    "        for entry in data[1:]:\n",
    "            apct = entry.split(' ')\n",
    "            kdata[i][ data[0] ][ apct[0][0:-1] ] = float(apct[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scores = [ 0.0 for i in range(len(clusters)) ]\n",
    "\n",
    "for k, v in kdata.items():\n",
    "    \n",
    "    score = 0.0\n",
    "    length = 0\n",
    "    for a, b in v.items():\n",
    "        nums = [ float(d) for c, d in b.items() ]\n",
    "        score += sum(nums) / float(len(nums))\n",
    "        length += 1\n",
    "    score = score / length\n",
    "    \n",
    "    scores[k] = score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i, score in enumerate(scores):\n",
    "    print('{}: {}'.format((i + 1) * 100, score))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// end gross k scoring //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// begin seed list creation helpers //"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('../../Thesaurus/thesaurus.pkl', 'rb') as f:\n",
    "    thesaurus = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "synSet = set()\n",
    "for k in thesaurus:\n",
    "    synSet.add(k.lower())\n",
    "    \n",
    "vocabSet = set()\n",
    "for k in wv.vocab:\n",
    "    vocabSet.add(k.lower())\n",
    "    \n",
    "intSet = synSet & vocabSet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "thesausets = [ set(ws) for ws in fullbags ]\n",
    "for i in range(len(thesausets)):\n",
    "    thesausets[i] = thesausets[i] & intSet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// end seed list creation helpers //"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
